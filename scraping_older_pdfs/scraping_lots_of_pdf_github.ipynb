{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping all PDFs before the API came into work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.webdriver import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "driver = webdriver.Chrome(options=options)  \n",
    "\n",
    "# Putting the downloaded PDFs in the correct folder\n",
    "downloads_folder = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "pdf_save_folder = os.path.join(downloads_folder, \"debates_pdf\")\n",
    "year_folder = os.path.join(pdf_save_folder, \"etc\")\n",
    "\n",
    "# Checking if the folder exists, otherwise creating it\n",
    "os.makedirs(year_folder, exist_ok=True)\n",
    "\n",
    "# Opening target webpage\n",
    "base_url = \"https://zoek.officielebekendmakingen.nl/resultaten?q=(c.product-area==%22officielepublicaties%22)and((dt.creator==%22Tweede%20Kamer%20der%20Staten-Generaal%22)or(dt.creator==%22Tweede%20Kamer%20OCV%20/%20UCV%22))and(((w.publicatienaam==%22Agenda%22))or((w.publicatienaam==%22Handelingen%22))or((w.publicatienaam==%22Kamerstuk%22))or(((w.publicatienaam==%22Kamervragen%20(Aanhangsel)%22)or(w.publicatienaam==%22Kamervragen%20zonder%20antwoord%22)))or((w.publicatienaam==%22Niet-dossierstuk%22)))%20AND%20dt.type==%22Handeling%22%20AND%20w.vergaderjaar==%222020-2021%22&zv=&pg=10&col=AlleParlementaireDocumenten&svel=Kenmerkendedatum&svol=Oplopend\"\n",
    "driver.get(base_url)\n",
    "\n",
    "# Waiting for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Implementing a maximum amount of pages to scrape for disk space purposes\n",
    "page_count = 0\n",
    "max_pages = 2\n",
    "\n",
    "# Looping through all pages until all \"Handelingen\"-documents per year have been processed\n",
    "while page_count < max_pages:\n",
    "    # Creating an empty list to store all PDF download links for this webpage\n",
    "    pdf_links = []\n",
    "    \n",
    "    try:\n",
    "        # Locating the \"Publicaties\"-element that contains the Download PDF button\n",
    "        publicaties_section = driver.find_element(By.ID, \"Publicaties\")\n",
    "\n",
    "        # Find all li-elements inside the first ul-element, which equals the amount of downloadable documents on this pages\n",
    "        li_elements = publicaties_section.find_elements(By.TAG_NAME, \"li\")\n",
    "\n",
    "        # Appending all Download-links to the previously defined list\n",
    "        for li in li_elements:\n",
    "            try:\n",
    "                # Locating the nested <ul class=\"result--actions\">-element\n",
    "                actions_ul = li.find_element(By.CLASS_NAME, \"result--actions\")\n",
    "\n",
    "                # Locating the relevant li-element inside the previously defined nested ul-element\n",
    "                action_li = actions_ul.find_element(By.TAG_NAME, \"li\")\n",
    "\n",
    "                # Locating the a-element that contains the href towards the PDF\n",
    "                pdf_link = action_li.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "\n",
    "                # Storing the PDF download link in the pdf_links-list\n",
    "                pdf_links.append(pdf_link)\n",
    "\n",
    "            # Catching off errors in the previous try-function, for debugging purposes\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping an item due to an error: {e}\")\n",
    "\n",
    "    # Catching off errors in the previous try-function, for debugging purposes\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding elements: {e}\")\n",
    "\n",
    "    # Downloading all PDFs in the pdf_links-list\n",
    "    for index, pdf_url in enumerate(pdf_links):\n",
    "        try:\n",
    "            # Printing the progress of downloading all PDFs on the page\n",
    "            print(f\"Downloading PDF {index + 1}/{len(pdf_links)}: {pdf_url}\")\n",
    "\n",
    "            # Downloading the PDF using requests\n",
    "            response = requests.get(pdf_url, stream=True)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                # Creating a filename utilising the download link\n",
    "                pdf_filename = os.path.join(year_folder, pdf_url.split(\"/\")[-1])\n",
    "\n",
    "                # Saving the downloaded PDF in the correct folder\n",
    "                with open(pdf_filename, \"wb\") as pdf_file:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        pdf_file.write(chunk)\n",
    "\n",
    "                # Printing if the saving was succesful\n",
    "                print(f\"PDF saved: {pdf_filename}\")\n",
    "\n",
    "            # Printing if the saving failed for debugging purposes\n",
    "            else:\n",
    "                print(f\"Failed to download PDF {index + 1}\")\n",
    "\n",
    "            # Short delay before the next download, to not hit quota limits\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Catching off errors in the previous try-function, for debugging purposes\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading PDF {index + 1}: {e}\")\n",
    "\n",
    "    # Trying to find and click the \"Next\"-button\n",
    "    try:\n",
    "        # Finding the relevant section and saving the button itself\n",
    "        pagination_section = driver.find_element(By.ID, \"paging-results\")\n",
    "        next_button = pagination_section.find_element(By.CLASS_NAME, \"next\").find_element(By.TAG_NAME, \"a\")\n",
    "\n",
    "        # If the button exists, click to go to next page\n",
    "        if next_button:\n",
    "            next_page_url = next_button.get_attribute(\"href\")\n",
    "            print(f\"Going to next page: {next_page_url}\")\n",
    "            page_count += 1\n",
    "            driver.get(next_page_url)\n",
    "            time.sleep(2)\n",
    "        \n",
    "        # Otherwise break down the loop (no extra pages to be found)\n",
    "        else:\n",
    "            print(\"No more pages. Exiting.\")\n",
    "            break\n",
    "    \n",
    "    # Catching off errors in the previous try-function, for debugging purposes\n",
    "    except Exception:\n",
    "        print(\"No 'Next' button found. Exiting loop.\")\n",
    "        break\n",
    "\n",
    "# Closing the browser after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making PDFs smaller, so to not overload my PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Define paths\n",
    "gs_path = r\"[put path to your Ghostscript executable here]\"\n",
    "input_folder = r\"[put path to the normally downloaded PDFs here]\"\n",
    "output_folder = r\"[put path to the desired output folder for the compressed PDFs here]\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Compression settings: Change \"/screen\" to \"/ebook\" or \"/printer\" for better quality\n",
    "compression_level = \"/screen\"\n",
    "\n",
    "# Process each PDF file\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        input_file = os.path.join(input_folder, filename)\n",
    "        output_file = os.path.join(output_folder, f\"compressed_{filename}\")\n",
    "\n",
    "        command = [\n",
    "            gs_path, \"-sDEVICE=pdfwrite\", \"-dNOPAUSE\", \"-dBATCH\", \"-dSAFER\",\n",
    "            f\"-dPDFSETTINGS={compression_level}\", f\"-sOutputFile={output_file}\", input_file\n",
    "        ]\n",
    "\n",
    "        subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(f\"Compressed: {filename} â†’ {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
